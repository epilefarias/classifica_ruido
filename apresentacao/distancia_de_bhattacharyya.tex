\documentclass{sbrt2017port}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage[utf8x]{inputenc}


\begin{document}

\title{As Medidas de Divergência e a Distância de Bhattacharyya em Seleção de Sinais}

\author{Thomas Kailath}
%\thanks{Maria da Silva e José da Silva?Faculdade de Tecnologia, Universidade Estadual de Campinas, Limeira-SP, Brasil, E-mails: maria@unicamp.br,jose@unicamp.br. Este trabalho foi parcialmente financiado pelo CNPq (XX/XXXXX-X).} }

\maketitle

\markboth{XXXV SIMPÓSIO BRASILEIRO DE TELECOMUNICAÇÕES E PROCESSAMENTO DE SINAIS - SBrT2017, 3-6 DE SETEMBRO DE 2017, SÃO PEDRO, SP} {XXXV SIMPÓSIO BRASILEIRO DE TELECOMUNICAÇÕES E PROCESSAMENTO DE SINAIS - SBrT2017, 3-6 DE SETEMBRO DE 2017, SÃO PEDRO, SP}

\begin{resumo}
A minimização da probabilidade de erro para determinar sinais ótimos é por vezes difícil de se obter. Consequentemente, várias medidas de desempenho subótimas que são mais fáceis de se obter e manipular têm sido estudadas. Neste artigo parcialmente tutorial, nós comparamos as propriedades de uma medida geralmente usada, a divergência, com uma nova medida que chamamos de Distância de Bhattacharyya. Esta nova medida de distância é por vezes mais fácil de se obter que a divergência. Nos problemas em que trabalhamos, ela dá resultados que são pelo menos tão bons quanto, e às vezes melhores, que os dados pela divergência.
\end{resumo}

\begin{chave}
Teoria da Comunicação, Detecção, Sinais Digitais, Enfraquecimento, Filtros Combinados, Relação Sinal-Ruído.
\end{chave}

\begin{abstract}
Minimization of the error probability to determine optimum signals is often difficult to carry out. Consequently, several suboptimum performance measures that are easier than the error probability to evaluate and manipulate have been studied. In this partly tutorial paper, we compare the properties of an often used measure, the divergence, with a new measure that we have called the Bhattacharyya distance. This new distance measure is often easier to evaluate than the divergence. In the problems we have worked, it gives results that are at least as good as, and are often better, than those given by the divergence.
\end{abstract}

\begin{keywords}
Communication Theory, Detection, Digital Signals,

Fading, Matched Filters, Signal-to-Noise Ratio.
\end{keywords}

\section{Introdução} \label{sec1}

%In problems of communication and radar, the optimum signals are those that minimize the probability of error.
Em problemas de comunicação e radar, os sinais ótimos são aqueles que minimizam a probabilidade de erro.
%However, in many cases, direct minimization of the probability of error so as to determine an optimum signal set is often impossible.
 Entretando, em muitos casos a minimização da probabilidade de erro para determinar um conjunto de sinais ótimos é normalmente impossível.
%This may be because an explicit analytical expression for the error probability is too difficult to find, or even if it can be found, the expression may be too complicated for analytical or numerical minimization. 
Isso pode ser porque uma expressão analítica explícita para o erro de probabilidade é muito difícil de encontrar e, mesmo se a expressão puder ser encontrada, ela pode ser muito complicada para minmização numérica ou analítica.
% Therefore, it is useful to search for signal selection criteria that may be weaker than the error probability but that are easier to evaluate and manipulate.
Portanto, é útil procurar por critérios de seleção de sinal que podem ser mais fracos que a probabilidade de erro mas são mais fáceis de encontrar e manipular.
% Maximizing a deflection ratio or a signal-to-noise (SNR) is an example of such a weaker criterion - in general, maximizing the SNR may not lead to the minimum error probability, but the SNR may still be used because it is easier to analyze and does provide some guidance in many cases.
Maximizar a taxa de defleção a relação sinal-ruído (SNR) são exemplos de critérios mais fracos - em geral, maximizar a SNR pode não levar à menor probabilidade de erro, mas a SNR ainda pode ser usada porque é mais fácil de analisar e provê algum direcionamento em muitos casos.
% In the search for suitable criteria, the notion of a distance between two probability distributions is quite useful.
Na busca por critérios aceitáveis, a noção de uma distância entre duas distribuições de probabilidade é razoavelmente útil.
% The two distributions will be the distributions of the observations in a binary hypothesis testing situation and the further apart we can make these distributions, hopefully the smaller will be the probability of mistaking one for the other. Therefore, various distance measures have been studied as simpler substitutes for the error probability.
As duas distribuições serão distribuições das observações em uma hipótese binária testando a situação e quanto mais separadas forem estas distribuições, esperamos que menor será a probabilidade de confundir uma com a outra. Portanto, várias medidas de distância têm sido estudadas como substitutas mais simples para a probabilidade de erro.


%What we would like for such distance measures is a property of the following type: if the distance between the two distributions is greater for a signal set \alpha than for a signal set \beta, then the error probability for set \alpha is always less than the error probability for set \beta.
O que nós gostaríamos para tais medidas de distância é uma propriedade como nos moldes de: se a distância entre as duas distribuições é maior para um conjunto de sinais $\alpha$ que para um conjunto $\beta$, então a probabilidade de erro para o conjunto $\alpha$ é sempre menor que para o conjunto $\beta$.
% This is too much to hope for-we shall only be able to find weaker relations between distance measures and the probability of error.
Isto é bom demais para se ter esperança - nós só podemos encontrar relações mais fracas entre medidas de distância e a probabilidade de erro.

%In statistics, the use of such distance measures has a long history beginning with the work of Pearson (as quoted in Tildesley [l]).
Em estatística, o uso destas medidas de distância tem um longo histórico começando com o trabalho de Pearson (como citado em Tildesley \cite{r1}).
% Two widely used distance measures in statistics are the D2-statistic of Mahalanobis [2], [3] and the linear discriminant function introduced by Fisher [4].
Duas medidas de distância amplamente usadas em estatística são a estatística-$D^2$ de Mahalanobis \cite{r2}, \cite{r3} e a função discriminante linear introduzida por Fisher \cite{r4}.
% These measures are discussed in detail in many statistical texts, e.g., Anderson [5] or Rao [6]. With the advent in 1948 of Shannon?s information theory, the divergence, a measure closely related to Shannon?s logarithmic measure of information, has become popular, though it had first been proposed before Shannon?s work by Jeffreys [7]. Several properties and applications of the divergence are noted in Kullback [8].
Estas medidas são discutidas em detalhe em muitos textos estatísticos, e.g., Anderson \cite{r5} ou Rao \cite{r6}. Com o advento da teoria de informação de Shannon em 1948, a divergência, uma medida fortemente relacionada com a medida de informação logarítmica de Shannon, ficou popular, apesar de ter sido proposta antes do trabalho de Shannon por Jeffreys \cite{r7}. Várias propriedades e aplicações da divergência são notadas em Kullback \cite{r8}.

%In the engineering literature the divergence has recently been applied to several problems with varying degrees of success (as seen in Marill and Green [9], Grettenberg [10], Hingorani [11], and others). In this paper, we propose another criterion for signal selection based on a statistic first introduced in a statistical $context^2$ by Bhattacharyya [12]. In the various signal selection problems we have considered so far, the Bhattacharyya distance has turned out to give better results than the divergence. Therefore, it seems like a useful measure to study.
Na literatura, a divergência tem sido recentemmente aplicada em vários problemas com variado grau de sucesso (como visto em Marill e Green \cite{r9}, Grettenberg \cite{r10}, Hingorani \cite{r11}, e outros). Neste artigo, propomos outro critério para seleção de sinal baseado em uma estatística introduzida primeiramente num contexto\footnote[2]{Outra aplicação estatística desta estatística foi feita por Kakutani \cite{r13}, que nota uma aparição anterior em um problema não estatístico (Hellinger \cite{r14}). Portanto, os nomes Hellinger e Kakutani são frequentemente associados com a estatística de Bhattacharyya.}
%Another early and well-known statistical application of this statistic was made by Kakutani [13], who notes an earlier appearance in a nonstatistical problem (Hellinger [14]). Therefore, the names Hellinger and 	Kakutani are often associated with the Bhattacharyya statistic. 
estatístico para Bhattacharyya \cite{r12}. Nos vários problemas de seleção de sinal que consideramos até agora, a distância de Bhattacharyya deu resultados melhores que a divergência. Portanto, parece uma medida útil para se estudar.

%It should be pointed out that the Bhattacharyya distance is actually a special case of a more general distance measure introduced by Chernoff [15]. This measure, which will be discussed briefly in Section III, is in general closer to the error probability than the Bhattacharyya distance. On the other hand, it is usually not as easy to evaluate as the Bhattacharyya distance, which, for this reason, will be preferred in this paper.
Poderíamos apontar que a distância de Bhattacharyya é na verdade um caso especial de uma distância mais geral introduzida por Chernoff \cite{r15}. Esta medida, que será discutida brevemente na Seção \ref{sec4}, é em geral mais próxima da probabilidade de erro que a distância de Bhattacharyya. Por outro lado, esta medida não é tão fácil de se calcular quanto a distância de Bhattacharyya, que, por esta razão, terá preferência neste artigo.

%We shall first briefly review some of the properties of the divergence and of the Bhattacharyya distance and shall then compare the two measures in various ways. 
Primeiramente, iremos revisar brevemente algumas das propriedades da divergência e da distância de Bhattacharyya e então iremos comparar as duas medidas de várias maneiras.

  %We shall confine ourselves largely to two-hypothesis problems. The two hypotheses mill be denoted hi, i = 1,2; and we shall set pi(x)dx = probability of the observation x when hypotheses hi is true. The observation x will, for simplicity be taken as an N-component vector. [The concepts can be extended to the case of continuous-time observations (See Section IV).]
Iremos nos limitar largamente a problemas com duas hipóteses. As hipóteses serão denotadas por $h_i, i = 1,2$; e nós determinamos $p_i(x)dx $ =  a probabilidade de uma observação $x$ quando a hipótese $h_i$ é verdadeira. A observação $x$ será tomada, por simplicidade, como um vetor de N componentes. [Os conceitos podem ser estendidos para o caso de observações no tempo contínuo (veja Seção \ref{sec5}).]

A taxa de similaridade:

$$ L(x) = \frac{p_1(x)}{p_2(x)}$$

%will play a prominent part in our discussions. In fact, both the divergence and the Bhattacharyya distance are convex functionals of the likelihood ratio.
terá papel importante em nossas discussões. De fato, tanto a divergência quanto a distância de Bhattacharyya são funcionais  convexas da taxa de similaridade.

\section{A divergência e a distância de Bhattacharyya} \label{sec2}

%The divergence was first introduced by Jeffreys [7], [lG]. It is defined as the difference in the mean values of the log-likelihood ratio under the two hypotheses.
A divergência foi introduzida inicialmente por Jeffreys \cite{r7,r16}. Ela é definida como a diferença nos valores médios da taxa de similaridade logarítmica sob duas hipóteses.

\begin{equation}
 J = E_1[\ln L(x)] - E_2[\ln L(x)]
 \label{eq1}
\end{equation}

onde

\begin{equation}
 E_i[\ln L(x)] = \int[\ln L(x)]p_i(x)dx, i = 1,2.
 \label{eq2}
\end{equation}

%These expectations of In L(z) are often called the Kullback-Leibler (K-L) numbers (Kullback and Leibler [17] and written
Estas expectativas de $\ln L(x)$ normalmente são chamadaas números de Kullback-Leibler (Kullback e Leibler \cite {r17}) e escritas

\begin{equation}
 I(1,2) = E_1[\ln L(x)], I(2,1) = -E_2[\ln L(x)].
 \label{eq3}
\end{equation}

Em geral 

\begin{equation}
 I(1,2) \neq I(2,1).
 \label{eq4}
\end{equation}

%The divergence is a symmetrized form of the K-L numbers
A divergência é uma forma simetrizada dos números de K-L

\begin{equation}
 J = I(1,2) + I(2,1).
 \label{eq5}
\end{equation}

%We shall calculate J for several examples in Section 11. The divergence satisfies all the postulates for a metric (distance) except the triangle inequality. That is we have J(1, 2) = J(2, l), J(1, 2) 2 0 with equality only when pl = pz but J(1, 2) + J(2, 3) may be less than J(1, 3).This last statement can be checked by taking pl, p2, p3 to be normal distributions with means zero and variances 1, 4, and 5.
Nós calcularemos $J$ em vários exemplos na Seção \ref{sec3}. A divergência satisfaz todos os postulados para uma métrica (distância) exceto o da desigualdade triangular. Que diz que nós temos $J(1,2) = J(2,1), J(1,2) \geq 0 $ com igualdade somente quando $p_1 = p_2$, mas $J(1,2) + J(2,3)$ pode ser menor que $J(1,3)$. Esta última frase pode ser checada tomando $p_1, p_2, p_3$ como distribuições normais com média 0 e variâncias 1, 4 e 5.

%Several other properties of the divergenceits additivity for independent observations, its behavior under transformations, relations to channel capacity, etc.,-and various applications to classification and hypothesis testing are given in IZullback [SI.
Várias outras propriedades da divergência - sua aditividade para observações independentes, seu comportamento sob transformações, suas relações com capacidade de canal, etc. - e várias aplicações em classificação e teste de hipóteses são dadas em Kullback \cite{r8}.

%For the present paper, we are interested in a property of the divergence that is related to signal selection. If we have two signal sets-or more generally, two sets of parameters for the densities p,(z), i = 1,2-say CY and p, we can order (or rank) then1 by means of the divergence. That is, we can say the signal set cy is better than the signal set p if J(a), the divergence between pl and p2 for parameters a! is larger than J(p). In communications and radar, the phrase ?signal set cy is better than set p? generally means ?the error probability (or more generally, the Bayes? risk) is less for signal set cy than it is for signal set p.? Now the error probability depends on the total distribution (and hence all the moments) of the likelihood ratio while the divergence depends only upon the first moments (mean values) of the likelihood ratio. Therefore, in general, if J(a) > J@) [Le., a: is better than @ under the divergence criterion] it will not be true that P,(a!) < P&). Consequently, the following theorem of Karlin and Bradt [I81 is quite surprising.
Para este artigo, estamos interessados na propriedade da divergência relacionada com a seleção de sinal. Se temos dois conjuntos de sinais - ou de forma mais geral, dois conjuntos de parâmetros para as densidades $p_i(x),i = 1,2$ - digamos $\alpha$ e $\beta$, podemos \textit{ordená-las} por meio da divergência. Ou seja, podemos dizer que o conjunto de sinais $\alpha$ é melhor que o conjunto $\beta$ se $J(\alpha)$, a divergência entre $p_1$ e $p_2$ para parâmetros $\alpha$ é maior que $J(\beta)$. Em comunicações e radar, a frase "o conjunto de sinais $\alpha$ é melhor que conjunto $\beta$" geralmente significa que "a probabilidade de erro (ou de forma mais geral, o risco de Bayes) é menor para o conjunto $\alpha$ do que para o sinal $\beta$". Agora a probabilidade de erro depende da distribuição total (e consequentemente de todos os momentos) da taxa de similaridade enquanto a divergência depende somente dos primeiros momentos (média) da taxa de similaridade. Portanto, em geral, se $J(\alpha) > J(\beta)
$ [i.e., $\alpha$ é melhor que $\beta$ sob o critério da divergência] não será verdade que $P_e(\alpha) < P_e(\beta)$. Consequentemente, o teorema de Karlin e Bradt \cite{r18} é surpreendente.


%If J(a) > J@), there exists a set of prior probabilities T = (TI, a2) for the two hypotheses, for which
Se $J(\alpha) > J(\beta)$, existe um conjunto de probabilidades a priori $\pi = (\pi_1,\pi_2)$ para as duas hipóteses, para os quais:

%a) < P,(P, (6)
\begin{equation}
 P_e(\alpha,\pi) < P_e(\beta,\pi)\footnote[3]{$P_e(\alpha,\pi)$ é a probabilidade de erro com o conjunto de parâmetros $\alpha$ e probabilidade a priori $\pi$}
 \label{eq6}
\end{equation}

%This is an interesting result, but it would be much more so if we could assert something more than just existence about the good set of prior probabilities. Unfortunately, this is hard to do. Nevertheless, Karlin and Bradt?s result encouraged Grettenberg [lo] to apply the divergence criterion to several signal selection problems. He was followed by several others.
Este é um resultado interessante, mas seria muito mais se pudéssemos afirmar algo mais do que apenas a existência de um bom conjunto de probabilidades a priori. Infelizmente, isto é difícil de fazer. Mesmo assim, o resultado de Karlin e Bradt encorajou Grettenberg \cite{r10} a aplicar o critério da divergência em vários problemas de seleção de sinais. Vários o seguiram.

%Before discussing these signal selection problems, let us introduce the Bhattacharyya distance. We first define the Bhattacharyya coefficient for two densities p,(x) by
Antes de discutirmos estes problemas de seleção de sinais, vamos introduzir a distância de Bhattacharyya. Primeiramente, definimos o coeficiente de Bhattacharyya para duas densidades $p_i(x)$ por

%p = Bhattacharyya coefficient = sdp1(z)p2(z)dz. (7)
\begin{equation}
 \rho = \text{coeficiente de Bhattacharyya } = \int \sqrt{p_1(x)p_2(x)dx}
 \label{eq7}
\end{equation}

%Clearly p lies between zero and unity. We can associate several distance measures with this coefficient. The one we shall use will be4
Claramente $\rho$ está entre 0 e 1. Nós podemos associar várias medidas de distância com este coeficiente. O que usaremos será \footnote[4]{Outra  possibilidade é a quantidade $\sqrt{1-\rho}$}

%B = the Bhattacharyya distance = -In p. (8)
\begin{equation}
 B = \text{a distância de Bhattacharyya } = -\ln \rho
 \label{eq8}
\end{equation}


%Clearly 0 5 B I a. It can be shown that B need not obey the triangle inequality (consider normal populations with means zero and variances 1, 4, and 5). (But does obey the triangle inequality.) We shall evaluate B for several distributions in Sections I1 and 111.
Claramente $0 \leq B \leq \infty$. Podemos mostrar que B não precisa obedecer a desigualdade triangular (considerando populações normais com médias zero e variâncias iguais a 1, 4 e 5). (Mas $\sqrt{1-\rho}$ obedece à desigualdade triangular). Nós iremos calcular $B$ para várias distribuições nas Seções \ref{sec3} e \ref{sec4}.

%The Bhattacharyya distance has many interesting properties, some of which we shall discuss later, but what led us to investigate its application to signal selection was the fact that B has the property discovered by Karlin and Bradt for the divergence.
A distância de Bhattacharyya tem várias propriedades interessantes, algumas das quais iremos discutir depois, mas o que nos levou a investigar sua aplicação na seleção de sinais foi o fato de que $B$ tem a propriedade descoberta por Karlin e Bradt para a divergência.

%If for two sets of system parameters a! and p, we have B(a) > B(P) or equivalently p(a) < p@), then there exists a set
Se, para dois ou mais conjuntos de parâmetros de sistema $\alpha$ e $\beta$, temos $B(\alpha) > B(\beta)$ ou de forma equivalente $\rho(\alpha) < \rho(\beta)$, então existe um conjunto

%a = (Tl, a2) (9)
\begin{equation}
 \pi = (\pi_1,\pi_2)
 \label{eq9}
\end{equation}

%of prior probabilities for which P,(a, T) < Pe@, T).
de probabilidades a priori para as quais $P_e(\alpha,\pi) < P_e(\beta,\pi)$.


%This result, as well as the earlier one of Karlin and Bradt for the divergence, follows easily from a theorem proved by Blackwell [19] and attributed by him to unpublished work by H. Bohnenblust, L. Shapley, and S. Sherman. We have
Este resultado, juntamente com o anterior de Karlin e Bradt para a divergência, segue facilmente um teorema provado por Blackwell \cite{r19} e atribuído por ele a um trabalho não publicado por H. Bohnenblust, L. Shapley, and S. Sherman. Nós temos

\subsection{Teorema (Blackwell)} \footnote[5]{Notamos que este teorema é atribuído a Karlin e Bradt por Grettenberg (compare com Teorema 1 de Grettenberg \cite{r10}), enquanto que o Teorema 2 de Grettenberg deveria de fato ter sido creditado a Karlin e Bradt.}

%P,(a, a) will be less than or equal to P,(p, n) for all a if and only if
$P_e(\alpha,\pi)$ será menor ou igual a $P_e(\beta,\pi)$ para todo $\pi$ se e somente se
 
\begin{equation}
 E_\alpha[\phi (L_\alpha)|h^{(2)}] \leq E_\beta[\phi (L_\beta)|h^{(2)}]
 \label{eq10}
\end{equation}

%for all continuous concave functions +(L). {E, [ h(2) 1 is the expectation of (La) under the hypothesis h(2)}.
para todas as funções côncavas contínuas $\phi(L)$. {$E_\alpha[\phi (L_\alpha)|h^{(2)}]$ é a esperança de $\phi(L)$ a hipótese $h^{(2)}$}.

%The proof of (9) is now simple. We note that dz is a concave function of L and that
A prova de (\ref{eq9}) agora é simples. Notamos que $\sqrt{L}$ é uma função côncava de $L$ e que

%P(4 = SdPl(Z, a)p2(z,ff )dz
\begin{equation}\label{eq11}
	\begin{split}
	\rho(\alpha) & = \int {\sqrt{p_1(x,\alpha)\sqrt{p_2(x,\alpha)dx}}} \\
	& = \int{\sqrt{\frac{p_1(x,\alpha)}{p_2(x,\alpha)}}p_2(x,\alpha)dx} = E_\alpha[\sqrt{L_\alpha}|h^{(2)}]
 	\end{split}
\end{equation}

%Similarly
Similarmente

$$ \rho(\beta) = E_\beta[\sqrt{L_\beta}|h^{(2)}]$$

%But then p@) < p(a) impliesEj31'dGlh(2)] E,[dE[h(2)], which contradicts (10). Therefore, Blackwell's theorem shows that if p(P) < p(a), then Pe(a, T) cannot be strictly smaller than P,@, T) for all T, which is just the result (9).
Mas então $\rho(\alpha) < \rho(\beta) $ sugere $E_\alpha[\sqrt{L_\alpha}|h^{(2)}]E_\beta[\sqrt{L_\beta}|h^{(2)}] $, o que contradiz \cite{r10}. Assim sendo, o teorema de Blackwell mostra que se $\rho(\alpha) < \rho(\beta) $, então $P_e(\alpha,\pi)$ não pode ser estritamente menor que $ P_e(\beta,\pi)$ para todo $\pi$, o que é o resultado (\ref{eq9}).

\section{Algumas aplicações em seleção de sinais} \label{sec3}

%Grettenberg [lo] used the comparison property (6) of the divergence J as a rationale For using J as a tool for signal selection. We have found that the Bhattacharyya distance B has a similar property (9) and this leads us to compare J and B as tools for signal selection. We shall consider three examples originally studied by the divergence method-two were treated by Grettenberg [lo] and one by Reiffen and Sherman [20].

Grettenberg \cite{r10} usou a propriedade de comparação (\ref{eq6}) da divergência $J$ como a racionalização para usar $J$ como uma ferramenta de seleção de sinais. Nós achamos que a distância de Bhattacharyya $B$ tem uma propriedade similar (\ref{eq9}) e isto nos leva a comparar $J$ e $B$ como ferramenta para seleção de sinal. Nós iremos considerar três exemplos estidados originalmente pelo método da divergência - dois foram tratados por Grettenberg \cite{r10} e um por Reiffen e Sherman \cite{r20}.

\subsection{Processos Gaussianos com Funções de Valor Médio Desiguais}

%The detection problem is to choose between the two hypotheses
O problema de detecção é escolher entre duas hipóteses

%hl:x(t) = ml(t) + n(t), h2:z(t) = m2(t) + n(t), teT
$$ h_1:x(t) = m_1(t) + n(t), h_2:x(t) = m_2(t) + n(t), t \in T$$

%where ml(t) and mz(t) are known functions of time, n(t) is a sample function of a zero mean Gaussian process with covariance function R(t, s), and ~(t) is the observation on the basis of which we are to decide whether hl or hz is true. For simplicity, we shall assume that the noise is white, with
onde $m_1(t)$ e $m_2(t)$ são funções conhecidas no tempo, $n(t)$ é uma função amostra de um processo Gaussiano de média zero e função de covariância $R(t,s)$, e $x(t)$ é uma observação na base da qual poderemos decidir entre $h_1$ e $h_2$ qual é verdadeira. Por simplicidade, iremos assumir que o ruído é branco, com

%2 R(t, s) = -6(t -
$$ R(t,s) = \frac{N_0}{2}\delta(t-s) $$

%and that the signals wzi(t), i = 1,2 have equal energies
e que os sinais $m_i(t), i=1,2$ têm energia igual

%Jnzi2(t)dt = E, i = 1,2 (12)
\begin{equation}
 \int_{T}^{} {m_i}^2(t)dt = E, i=1,2
 \label{eq12}
\end{equation}

%and a correlation coefficient p delined by
e um coeficiente de correlação $\mu$ definido por

\begin{equation}
 \int_{T}^{} m_1(t)m_2(t)dt = E\mu.
 \label{eq13}
\end{equation}

%Now some computation will show that
Agora alguns cálculos irão mostrar que

\begin{equation}
 B = \frac{1}{8}J=\frac{1}{8}\frac{2E}{N_0}(1-\mu)
 \label{eq14}
\end{equation}

%so that both the Bhattacharyya distance B and the divergence J are maximized by choosing the signals ?ni(t), i =1,2 to be antipodal, viz., such that
tais que tanto a distância de Bhattacharyya $B$ quanto a divergência $J$ são maximizadas ao se escolher os sinais $m_i(1), i=1,2$ que sejam antipodais, ou seja

\begin{equation}
 m_1(t) = -m_2(t) \quad ou \quad \mu = -1
 \label{eq15}
\end{equation}

%It is well-known that this choice of signals is in fact theone that minimizes the error ]probability for all prior probabilities 11, so that in this problem both the divergence and the Bhattacharyya distance signal selection criteria yield signals that are in fact also optimum on an error probability basis. This happy coincidence is unfortunately not universal, as the next example shows.
É amplamente conhecido que esta escolha de sinais é de fato a que minimiza a probabilidade de erro para todas as probabilidades a priori, tal que neste problema tanto o critério de seleção de sinais da divergência quanto o da distância de Bhattacharyya produzem sinais que são de fato também ótimos com base na probabilidade de erro. Esta feliz coincidência infelizmente não é universal, como o próximo exemplo mostra.

\subsection{Processos Gaussianos com Funções de Covariância Desiguais}

%The problem is the following: one of two signals m,(t), i = 1,2 each of energy E/L
O problema é o seguinte: um dos dois sinais $m_i(t), i=1,2$ cada um com energia $E/L$

\begin{equation}
 \int_{T}^{} {m_i}^2(t)dt = E/L, i=1,2
 \label{eq16}
\end{equation}

%is transmitted over L channels in each of which the signal is perturbed multiplicatively by ffk and additively by nk(t), so that the received signals are
é transmitido por $L$ canais onde em cada um o sinal é perturbado multiplicativamente por $\alpha_k$ e aditivamente por $n_k(t)$, tal que os sinais recebidos são

\begin{equation}
 h^{(i)}:x_k(t)  = \alpha_km_2(t) + n_k(t), k = 1,...,L, i = 1,2 \footnote[6]{No problema de comunicação, é fisicamente mais razoável é considerar $\alpha_k, x_k(t), m_2(t) e n_k(t)$ envelopes comlexos dos sinais reais correspondentes (de banda estreita). Por comparação com os resultados iniciais de Pierce \cite{r21} e de Grettenberg \cite{r10}, nós assumiremos que o são. Entretanto $B$ e $J$ podem ser calculados e otimizados quer os sinais sejam de banda estreita ou não.}
 \label{eq17}
\end{equation}

%We shall assume that the { ak f are independent identically distributed Gaussian random variables with
Vamos assumir que ${\alpha_k}$ são variáveis aleatórias Gaussianas independentes e identicamente distribuidas com

\begin{equation}
 E[\alpha_k] = 0,E[\alpha_k\alpha_j] = \sigma^2\delta_{kj}, k,j = 1,...,L
 \label{eq18}
\end{equation}

%The additive noises will be assumed to be Ga,ussian, independent, identically distributed with
Assumiremos que os ruídos aditivos serão Gaussianos, independentes e identicamente distribuidos com

\begin{equation}
 E[n_k(t)] = 0,E[n_k(t)n_j(s)] = [\frac{N_0}{2}\delta(t-s)]\delta_{kj}
 \label{eq19}
\end{equation}

%and independent of the random variables [ak]
e independentes das variáveis aleatórias $[\alpha_k]$

\begin{equation}
 E[\alpha_k] = 0,E[\alpha_kn_k(t)] = 0, k,j = 1,...,L
 \label{eq20}
\end{equation}

%The signal selection problem in the above is the following: if we had just a single link, there is a large probability that the transmitted signal will be drastically weakened by the multiplicative noise assuming a low value. To combat this loss of signal, which is usually called the loss due to fading, a classical solution (1923-1927) is to use a nuniber of links to provide diversity. If we use a number of links with independent multiplicative noises, the probability that the signal will be simultaneously reduced on all links can be made quite small. It would therefore seem desirable to use as many diversity links as possible. However, another factor has to be taken into account. The total signal energy is usually constrained, say to E', and therefore, the more links we use the less will be the ava,ila.ble signal energy per link. If too many links are used, t.he signal energy per link may be so small that the result,ing greater degradation in the performance of each link cannot be compensated for by the diversity gain provided by the use of the larger number of links. There should therefore be an optimum number of links to use for a given total signal energy E.
O problema na seleção de sinais exposta acima é como segue: se nós tivermos apenas um canal, há uma alta probabilidade do sinal transmitido ser drasticamente enfraquecido pelo ruído multiplicativo assumindo um valor baixo. Para combater esta perca de sinal, que normalmente é chamada 'perda devido ao esvanecimento', uma solução clássica (1923 - 1927) é usar um número de canais para prover diversidade. Se nós usarmos um número de canais com ruídos multiplicativos independentes, a probabilidade de que o sinal vai ser simultaneamente reduzido em todos os links pode se tornar pequena. Seria, portanto, desejável usar a maior diversidade de canais possível. Contudo, outro fator deve ser levado em consideração. A energia total do sinal é normalmente restrita digamos, a um valor $E$ e, assim sendo, quanto mais canais usarmos menor vai ser a energia de sinal disponível por canal. Se muitos canais são usados, a energia de sinal por canal pode ser tão pequena que a maior diversidade de ganho provida 
pela diversidade de canais não compensará a maior degradação resultante no desempenho de cada canal. Portanto deve haver um número ótimo de canais para usar dado uma energia total de sinal $E$.

%Let us now see what results the use of the divergence and Bhattacharyya measures yield. First for the divergence, some computation shows that; if (for simplicity) we make t,he additional assumption that the signals are orthogonal, viz.,
Vejamos agora quais resultados obtemos com o uso da divergência e de Bhattacharyya. Primeiramente para a divergência, alguns cálculos mostram que, se (por simplicidade) nós assumimos adicionalmente que os sinais são ortogonais, ou seja

\begin{equation}
 \int_{0}^{T} m_1(t)m_2(t)dt = 0
 \label{eq21}
\end{equation}

%then the divergence is
então a divergência é

\begin{equation}
 J = \frac{R^2}{R+L}, R = 2E\sigma^2/N_0
 \label{eq22}
\end{equation}

%And clearly for a fixed E, this is maximized by setting L = 1, that is by putting all the energy in a single link. However, our previous discussion indicates that this is not a very reasonable thing to do when R is large. And in fact, by the use of certain bounding techniques and by numerical evaluation, Pierce [21] found the optimum number of links as a function of the SNR, R = 2Eu2/N,, for the case of equal prior proba,bilities. Pierce showed that for low SNR R << 1 the use of a single link is optimum but that for large SNR, R >> 1, the optimum number of links is given (to a very good approximation) by the remarkably simple formula7
E claramente para um $E$ fixado, isso é maximizado estabelecendo $L=1$, o que significa colocar toda a energia em um único canal. Entretanto, nossa discussão anterior indica que isto não é algo razoável de se fazer quando $R$ é grande. E de fato, utilizando certas técnicas de delimitação e por estimação numérica, Pierce \cite{r21} encontrou o número ótimo de canais como uma função da SNR, $ R = 2E\sigma^2/N_0 $, para o caso de probabilidades a priori iguais. Pierce mostrou que para um baixo SNR $R << 1$ o uso de um único canal é ótimo, mas para um alto SNR $R >> 1$, o número ótimo de caanais é dado (aproximadamente) por uma fórmula notavelmente simples \footnote [7] {Grettenberg \cite{r10} 	erroneamente afirma que a probabilidade de erro para este caso diminui monotonicamente com $L$, tal que a melhor escolha é $L = \infty$}

\begin{equation}
 L_{opt} = R/3
 \label{eq23}
\end{equation}

%where [.x] means ?the greatest integer less than or equal to x.? We conclude from this that in Grettenberg?s words ([lo], p. 275), ?the range of source statistics for which the maximum divergence code has a lower error probability than a different code does not always include the equiprobable case.? We should repeat that the Iiarlin-Bradt theorem (6) tells us that the maximum divergence solution (L = 1) must be best, no matter how large the SNR is, for some set of prior probabilities. Unfortunately, the theorem gives us no information about this good set of prior probalilities and in many problems it might be that this set is highly skewed [TI = unity] and therefore highly improbable.
onde $[x]$ significa "o maior número inteiro menor ou igual a $x$". A conclusão que extraímos disto é que, tomando as palavras de Grettenberg (\cite{r10}, p. 275), "a amplitude de estatísticas de fonte para as quais o código de máxima divergência tem a menor probabilidade de erro que um código diferente nem sempre inclui o caso equiprovável." Nós devemos repetir que o teorema de Karlin-Brandt (\ref{eq6}) nos mostra que a solução de máxima divergência ($L=1$) deve ser a melhor, não importa quão grande é a SNR, para um determinado conjunto de probabilidades a priori. Infelizmente, o teorema não nos dá nenhuma informação sobre esse conjunto de probabilidades bom e em muitos problemas pode ser que este conjunto seja altamente distorcido [$\pi_1$ = unidade] e, portanto, altamente improvável.

%The Bhattacharyya distance in this problem can be calculated to be
A distância de Bhattacharyya neste problema pode ser calculada como

$$ \rho = e^{-B} = {(1+\frac{R}{L})}^L/{(1+\frac{R}{2L})}^{2L}, R = \frac{2E\sigma^2}{N_0}$$

%Differentiation shows that B is maximum when
A diferenciação nos mostra que $B$ é máximo quando

$$ \frac{x^2}{(x+1)(x+2)} = \ln{\frac{x^2 + 4x + 4}{4x + 4}}, x = R/L $$

%and computer solution of this equation yields
e a solução computacional desta equação fornece

$$ L_{opt} = R/3.07 $$

%which is the same as the solution (23) found by Pierce [21] from the exact expression for P,, which incidentally is
o que é a mesma da solução \ref{eq23} encontrada por Pierce \cite{r21} da expressão para $P_e$, que incidentalmente é

$$ P_e = \frac{1}{{(2+R/L)}^L}\sum_{k=0}^{L-1} \left(\begin{array}{c} L-1+k \\ k \end{array} \right){\left(\frac{1+R/L}{2+R/L}\right)}^k $$

%It can be shown [see discussion after (49)] that asymptotically the Bhattacharyya distance essentially determines the error probability in the sense that 
Podemos demonstrar [veja discussão após (\ref{eq49})] que assintoticamente a distância de Bhattacharyya essencialmente determina a probabilidade de erro em um sentido que

%lim Pe = c.cR, c = a constant.
%pe+O

$$ \lim_{P_e\rightarrow 0} P_e = c . e^{-B}, c = uma \quad constante $$

%Thus for the (zero-mean) Gaussian signal in Gaussian noise problem, the Bhattacharyya distance yields better results (at least for the situation of most interest in communication problems where TI A T?) than the divergence.
Portanto, para um sinal Gaussiano (de média zero) no problema de ruído Gaussiano, a distância de Bhattacharyya fornece melhores resultados (pelo menos para a situação de maior interesse em problemas de comunicação onde $\pi_1 \dot{=} \pi_2$) que a divergência.

%Gaussian processes with different means and covariance yield similar results-the divergence and Bhattacharyya distance criteria yield similar results at low SNR,* but at high SNR they give different solutions, with the divergence solution being poorer (at least for r1 A aa).
Processos Gaussianos com diferentes médias e covariância fornecem resultados similares - a divergência e a distância de Bhattacharyya fornecem resultados similares em uma baixa SNR\footnote[8]{É fácil checar neste problema que em uma baixa SNR, $J \dot{=} 8B $ [para qualquer valor de SNR, nós mostraremos (\ref{eq72}) que para densidades Gaussianas $ J \geq 8B $.}, mas com uma alta SNR eles fornecem soluções diferentes, com a solução dada pela divergência sendo mais pobre (ao menos para $\pi_1 \dot{=} \pi_2$).

%However, as a last example, we treat a problem in which the two criteria again yield similar results.
Contudo, como último exemplo, tratamos um problema em que os dois critérios fornecem resultados similares.

\subsection{Processos de Poisson com Funções de Valor Médio Desiguais}

%The detection problem is to choose between two Poisson processes whose mean value functions are
O problema de detecção é escolher entre dois processos de Poisson cujas funções de valor médio são

\begin{equation}
 m^{(k)}(t) = {\lambda_i}^{(k)} + \lambda_0, (i-1)\bigtriangleup \leq t \leq i\bigtriangleup, i = 1,...,N k = 1,2
 \label{eq24}
\end{equation}

%where the observation interval T has been divided into N intervals of length A, NA = T. The {X,@) 1 are determined by our choice of signals. We constrain this choice by
onde o intervalo de observação $T$ foi dividido em $N$ intervalos de comprimento $\bigtriangleup, N\bigtriangleup = T$. Os \{${\lambda_i}^{(t)}$\} são determinados pela nossa escolha de sinais. Nós restringimos essa escolha por

\begin{equation}
 {\lambda_i}^{(k)} \geq 0, \sum \bigtriangleup({\lambda_i}^{(1)} + {\lambda_i}^{(2)}) = 2E
 \label{eq25}
\end{equation}

%The observation is the sequence (n< 1 of the number of events (photon arrivals at a photon detector) occurring in the several intervals; the number of events in the ith interval is Poisson distributed with
A observação é a sequência \{$n_i$\} do número de eventos (chegada de fóton em um detector de fótons) ocorridos em vários intervalos. o número de eventos no $i$ésimo intervalo de Poisson é distribuido com \footnote[9]{Por conveniência, deste ponto em diante consideramos $\bigtriangleup = 1$}

\begin{equation}\label{eq26}
	\begin{split}
 	P^{(k)}  \{ n_i \text{ eventos em } & (i-1)\bigtriangleup \leq t \leq i\bigtriangleup  \} \\
 	 = \frac{{[({\lambda_i}^{(k)} + \lambda_0)\bigtriangleup]}^{n_i}}{n_i!} e^{-\bigtriangleup}({\lambda_i}^{(k)} + \lambda_0)
 	\end{split}
\end{equation}

%The numbers of events in disjoint intervals are assumed to be independently distributed. For this problem, the divergence J, and Bhattacharyya distance B can be shown to be
Assumimos que os números de eventos em intervalos desjuntos são independentemente distribuidos. Para este problema, a divergência $J$ e a distância de Bhattacharyya podem ser dadas por

\begin{equation}\label{eq27}
	\begin{split}
 	J = & \sum_{1}^{N} ({\lambda_i}^{(1)} + \lambda_0) \ln \left[ \frac{({\lambda_i}^{(1)} + \lambda_0)}{({\lambda_i}^{(2)} + \lambda_0)} \right] \\
 	 & + \sum_{1}^{N} ({\lambda_i}^{(2)} + \lambda_0) \ln \left[ \frac{({\lambda_i}^{(2)} + \lambda_0)}{({\lambda_i}^{(1)} + \lambda_0)} \right]
 	\end{split}
\end{equation}

\begin{equation}
 B = 2\sum_{1}^{N} {\left[\sqrt{{\lambda_i}^{(1)} + \lambda_0} - \sqrt{{\lambda_i}^{(2)} + \lambda_0} \right]}^2
 \label{eq28}
\end{equation}


%Let us first determine the choice of signals (i.e., of the 1X,@)) that maximizes J. A first step is clearly to make
Iremos primeiramente determinar a escolha de sinais (ou seja, dos $ {\lambda_i}^{(k)}$) que maximizam $J$. O primeiro passo é claramente fazer

\begin{equation}
 {\lambda_i}^{(k)} = 0, quando \quad {\lambda_i}^{(j)} \neq 0, j\neq k
 \label{eq29}
\end{equation}

%i.e., to make the signals orthogonal in a certain sense. However, the divergence can be increased still further. Thus, let us define a sequence
ou seja, fazer os sinais ortogonais em um certo sentido. Entretanto, a divergência pode ser ainda mais incrementada. Portanto, definamos a sequência

\begin{equation}
	q_i = {\lambda_i}^{(1)} se {\lambda_i}^{(2)} = 0, {\lambda_i}^{(1)} \geq 0
	\label{eq30}
\end{equation}

%The constraint (25) now is pi = 2E and the divergence is
A restrição (\ref{eq25}) agora é $ \sum_{1}^{N} q_i = 2E $ e a divergência é

% TEM UMA EQUAÇAO SEM NADA NO ORIGINAL SERIO 

\begin{equation}
	\quad
	\label{eq31}
\end{equation}

\begin{equation}
	\frac{1}{\lambda_0}J = \sum \frac{q_i}{\lambda_0}\ln \left( \frac{q_i}{\lambda_0} + 1 \right), \sum q_i = 2E
	\label{eq32}
\end{equation}

%A stationary point for the expression in (32) is easily found by the use of Lagrange multipliers to be
Um ponto fixo para a expressão em (\ref{eq32}) é facilmente encontrado usando os multiplicadores de Lagrange como

$$ q_i = 2E/N, i = 1,...,N $$

%However, this stationary point is a minimum rather than a maximum. We can show, that J is :t convex function of the qi so that the maximum of J will occur at any extreme point.
Contudo, este ponto fixo é mais provavelmente um mínimo do que um máximo. Nós podemos mostrar que $J$ é a função convexa de $q_i$ tal que o máximo de $J$ vai ocorrer em qualquer ponto extremo.

\begin{equation}
	q_i = 2E \quad para \quad alguns \quad i, q_i = 0, i \neq j
	\label{eq33}
\end{equation}

%Therefore, an optimum choice of signals (based on the divergence criterion) is
Portanto, uma escolha ótima de sinais (baseada no critério da divergência) é

\begin{equation}
	\{ {\lambda_i}^{(1)} \} = \{ 2E, 0,...,0 \}, \{ {\lambda_i}^{(2)} \} = \{ 0, 0,...,0 \}
	\label{eq34}
\end{equation}

%or any permutation of (44).
ou qualquer permutação de (\ref{eq44}).

%As we shall now show, maximiization of B yields the same result. With formula (2s) for B in mind, we note that
Como mostraremos agora, a maximização de $B$ fornece o mesmo resultado. Com a fórmula (\ref{eq28}) para $B$ em mente, notamos que

\begin{equation}\label{eq35}
	\begin{split}
	{\left[ \sqrt{{\lambda_i}^{(1)}+ \lambda_0} - \sqrt{{\lambda_i}^{(2)}+ \lambda_0} \right]}^2 \\
	= ( {\lambda_i}^{(1)}+ \lambda_0 ) {\left[ 1 - \sqrt{\frac{{\lambda_i}^{(1)}+ \lambda_0}{{\lambda_i}^{(2)}+ \lambda_0}} \right]}^2 \\
	\leq ( {\lambda_i}^{(1)}+ \lambda_0 ) {\left[ 1 - \sqrt{\frac{\lambda_0}{{\lambda_i}^{(1)}+ \lambda_0}} \right]}^2
	\end{split}
\end{equation}

%with equality if and only if Xi(2) = 0. The argument can be repeated with in place of X(2). Therefore, for a maximum of B, we must set
com igualdade se e somente se ${\lambda_i}^{(2)} = 0$; O argumento pode ser repetido com ${\lambda_i}^{(1)}$ no lugar de ${\lambda_i}^{(2)}$. Portanto, para maximizar $B$, devemos estabelecer

\begin{equation}
	{\lambda_i}^{(k)} = 0, se {\lambda_i}^{(j)} > 0, j \neq k
	\label{eq36}
\end{equation}

%This is also one of the conditions necessary for the maximization of the divergence J. Therefore, as in (30), let us define a sequence
Esta é também uma das condições necessárias para a maximização da divergência $J$. Portanto, como visto em (\ref{eq30}), vamos definir uma sequência

\begin{equation}
	q_i = \left\{\begin{matrix} {\lambda_i}^{(1)}, se {\lambda_i}^{(2)}=0, {\lambda_i}^{(1)} \geq 0 \\{\lambda_i}^{(2)}, se {\lambda_i}^{(1)}=0, {\lambda_i}^{(2)} \geq 0	\end{matrix}\right.
	\label{eq37}
\end{equation}

%with this the Bhattacharyya distance (28) can be written
com isso a distância de Bhattacharyya (\ref{eq28}) pode ser escrita como

\begin{equation}
	B = 2\sum_{1}^{N} {[ \sqrt{q_i + \lambda_0} - \sqrt{\lambda_0} ]}^2, \sum_{1}^{N}q_i = 2E
	\label{eq38}
\end{equation}

%and we have
e assim temos

%EQUACAO 39
\begin{equation}
	B \leq 2 [\sum_{1}^{N} \sqrt{q+\lambda_0} - \sqrt{\lambda_0} ]^2
	\label{eq39}
\end{equation}

%with equality if and only if all the terms in the sum (38) save one, say dpi + X, - v'L, are equal to zero. Therefore an optimum choice of signals (based on the Bhattacharyya distance criterion) is
com igualdade se e somente se todos os termos na soma (\ref{eq38}) menos um, digamos $\sqrt{q+\lambda_0} - \sqrt{\lambda_0}$ são iguais a zero. Portanto uma escolha ótima de sinais (baseada no critério da distância de Bhattacharyya) é

%EQUAÇÃO 40
\begin{equation}
	\{{\lambda_i}^{1}\} = \{2E, 0,...,0 \} \{{\lambda_i}^{2}\} = \{0, 0,...,0 \}
	\label{eq40}
\end{equation}
	
%or any permutation thereof.
ou qualquer permutação desta.

%Thus the optimum signals under the divergence and Bhattacharyya distance criteria are the same. Some computer studies by Blunlenthal (Stanford University) indicate that sharp pulse-like signals of the form (34) or (40) also minimize the error probability (for equally likely signals). We should point out that Reiffen and Sherman [ZO] suggested the optimality of signals llke (34) or (40) at low SNR'O by using essentially the divergence criterion. Abend [22] proved that similar signals maximized a suitably defined SNR at all signal levels.
Assim, os sinais considerados ótimos pelos critérios da divergência e da distância de Bhattacharyya são os mesmos. Alguns estudos computacionais por Blunlenthal (Stanford) indicam que sinais semelhantes a um pulso forte de forma (\ref{eq34}) ou (\ref{eq40}) também minimizam a probabilidade de erro (para sinais igualmente prováveis). Nós devemos apontar que Reiffen e Sherman \cite{r20} sugeriram que a otimização de sinais como (\ref{eq34}) ou (\ref{eq40}) em baixa relação sinal ruído \footnote[10]{Em baixa relação sinal ruído todas as medidas propostas parecem coincidir.} essencialmente usando o critério da divergência. Abend \cite{r22} provou que sinais similares maximizaram uma relação sinal ruído previamente definida em todos os níveis de sinal.

\section{Algumas outras propriedades de B e J} \label{sec4}


%In the last Section, we have shown in several problems that the Bhattacharyya distance is a good measure for signal selection. In this section, we shall present some additional properties of B, and pursue the comparison with the divergence J a little further. This section is largely a compilation of several results culled from the statistical literature. We begin with a convenient geometric interpretation of B or, actually, p = ecB.
Na última Seção, mostramos em vários problemas que a distância de Bhattacharyya é uma boa medida para seleção de sinais. Nesta seção, apresentaremos algumas propriedades adicionais de $B$, e traremos a comparação com a divergência $J$ um pouco mais longe. Esta seção é de forma geral uma compilação de vários resultados selecionados de literatura em estatística. Iremos começar com a conveniente interpretação geométrica de $B$ ou, na verdade, $\rho = e^{-B}$.

\subsection{Interpretação Geométrica}

%Bhattacharyya [la] proposed that the numbers { dpm, all allowable x, i = 1,2f, be regarded as the direction cosines of two vectors in the space of x. Alternatively, we can regard them as defining two points on the unit sphere (since.fp,(z)dx = 1, i = l,2) of x. The Bhattacharyya coefficient p can be regarded as the cosine of the angle between these two vectors, i.e., p= cos A, A = angle between the lines with direction cosines
Bhattacharyya \cite{r12} propôs que os números $ \{ \sqrt{p_i(x)}$, \quad todos \quad x \quad permissíveis,$ i=1,2 \} $ sejam considerados cossenos direcionais de dois vetores no espaço de $x$. Alternativamente, podemos considerá-los como dois pontos que definem a esfera unitária (como $ \int p_i(x)dx = 1, i = 1,2 $) de $x$. O coeficiente de Bhattacharyya $\rho$ pode ser considerado o cosseno do ângulo entre estes dois vetores, i. e., $ \rho = cos \bigtriangleup, \bigtriangleup = $ ângulo entre as linhas com cossenos em diferentes direções.

\begin{equation}
 \{ \sqrt{p_i(x)} \}
 \label{eq41}
\end{equation}


%The angle A must clearly be between 0 and T/Z and therefore 0 < p < 1. It is now natural to also consider the distance, say b, between the two points { m, i = 1,2 f on the unit sphere
O ângulo $\bigtriangleup$ deve claramente ser entre $0$ e $\pi/2$, portanto $0 < \rho < 1$. Agora é natural considerar também a distãncia, digamos $\partial$, entre dois pontos  $ \{ \sqrt{p_i(x)}, i=1,2 \} $ na esfera unitária

\begin{equation}
 \partial \triangleq \int {[\sqrt{p_1(x)} - \sqrt{p_2(x)}]}^2dx
 \label{eq42}
\end{equation}

%Clearly
Claramente

\begin{equation}
 \partial = 4 \sin \frac{\epsilon}{2} = 2(1-\cos \bigtriangleup) = 2(1-\rho)
 \label{eq43}
\end{equation}

%The distance b occurs in the work of Jeffreys [7], [16].
A distância $\partial$ ocorre no trabalho de Jeffreys \cite{r7,r16}.

\subsection{A Distância Variacional de Kolmogorov}

%Another distance measure closely related to p is the variational distance of Kolmogorov (see Adhikari and Joshi [23], p. 71); this is defined as
Outra medida de distância fortemente relacionada a $\rho$ é a distância variacional de Kolmogorov (veja Adhikari e Joshi \cite{r23}, p.71); ela é definida por

\begin{equation}
	K(\pi) = \frac{1}{2}\int (|\pi_1p_1(x)-\pi_2p_2(x)|dx)
	\label{eq44}
\end{equation}
%EQUACAO 44

%where a, and a2 are the prior probabilities of the hypotheses hl and hz. Kraft [24] has obtained certain important inequalities between p and K(.R). His inequalities (extended to the case a1 # aZ) are
onde $\pi$ e $\pi_2$ são as probabilidades a priori das hipóteses $h_1$ e $h_2$. Kraft \cite{r24} obteve certas desigualdades importantes entre $\rho$ e $K_{(\pi)}$. Suas desigualdades (estendidas para o caso de $\pi_1 \neq \pi_2$) são

\begin{equation}
	\sqrt{1-4\pi_1\pi_2\rho^2} \geq 2K(\pi) \geq 1 - \sqrt{\pi_1\pi_2\rho}
	\label{eq45}
\end{equation}
%EQUAÇÃO 45

%The first inequality follows via Schwarz's inequality from
A primeira desigualdade vem da desigualdade de Schwartz

%EQUAÇÃO SEM NUMERO
\begin{align*}
	[\int {|\pi_1p_1-\pi_2p_2|dx}]^2 \leq \int {|\sqrt{\pi_1p_1} - \sqrt{\pi_2p_2}|}^2dx. \times \\
	{|\sqrt{\pi_1p_1} - \sqrt{\pi_2p_2}|}^2dx = [1 - 2\sqrt{\pi_1\pi_2\rho}][1 + 2\sqrt{\pi_1\pi_2\rho}]
\end{align*}

%The second inequality follows from
A segunda desigualdade vem de 

%EQUAÇÃO SEM NUMERO
\begin{align*}
	\int {|\pi_1p_1-\pi_2p_2|dx} \geq \int {|\sqrt{\pi_1p_1} - \sqrt{\pi_2p_2}|}^2dx \\
	= 1 - 2\sqrt{\pi_1\pi_2\rho}
\end{align*}

%There is another useful way of writing K(n). A standard argument in analysis shows that
Há outro jeito útil de escrever $K(\pi)$. Um argumento padrão em análise mostra que

%EQUAÇÃO 46
\begin{equation}
	K(\pi) = \max_A |\pi_1Pr\{x \in A|h_1\}-\pi_2Pr\{x \in A|h_1\}|
	\label{eq46}
\end{equation}

%where the maximum is taken over all sets A in the space of x. This form for K(n) shows that it is closely related to the (optimum) probability of error. For, if we define A, = (xlh, is chosen when x is received )
onde o 	máximo é medido de todos os conjuntos $A$ no espaço de $x$. Esta forma de $K(\pi)$ é fortemente relacionada com a probabilidade de erro (ótima). Para, se definirmos $A_i = \{x | h_i$ é escolhido quando $x$ é recebido $\}$

%EQUAÇÃO MACETA 47 SÉRIO TEM 4 ANDARES ESSA PORRA
\begin{equation} \label{eq47}
	\begin{split}
	P_e & = \min_{A_1} \{ \pi_1 \int_{A_2} p_1(x)dx - \int_{A_1} p_2(x)dx \} \\
	& = \pi_1 - \max_{A_1} \{ | \int_{A_1} \pi_1p_1(x) - \pi_2p_2(x)dx| \} \\
	& = \pi_1 - \max_{A_1} |Pr\{x \in A_1|h_1\} \equiv \pi_2Pr\{x \in A_1|h_2\} | \\
	& = \pi_1 - K(\pi)
	\end{split}
\end{equation}

\subsection{Limites Superiores e Inferiores em Probabilidade de Erro}

%Combining (47) and the relations (45), we note that
Combinando (\ref{eq47}) e as relações (\ref{eq45}), notamos que

%EQUAÇÃO 48
\begin{equation} \label{eq48}
	\begin{split}
		2\pi_1 - \sqrt{1-4\pi_1\pi_2\rho^2} \leq 2P_e = & 2\pi_1  - K(\pi) \\
		& \leq 2\pi_1 - 2\sqrt{\pi_1\pi_2\rho}
	\end{split}	
\end{equation}

%If a1 = aZ = 1/2, this becomes
se $\pi_1 = \pi_2 = \frac{1}{2}$, isso se torna 

%EQUAÇÃO 49
\begin{equation}
	\frac{1}{4} \rho^2 \leq \frac{1}{2} (1-\sqrt{1-\rho}) \leq P_e \leq \frac{1}{2}\rho \footnote[11]{Se assumimos que $\pi_1 = \pi_2$ quando de fato eles não o são, nós só aumentamos a probabilidade de erro. Portanto, o limite superior em (\ref{eq49}) é verdadeiro pra quaisquer $\pi_1,\pi_2$}
	\label{eq49}
\end{equation}

%The bounds (49) are not entirely satisfactory. As p +. 0 (low PC) the difference between the upper and lower bounds tends to zero but their ratio tends to infinity. However, in an important special case the upper bound is exponentially best, i.e.,
Os limites (\ref{eq49}) não são inteiramente satisfatórios. À medida que $\rho \rightarrow 0$ (baixo $P_e$) a diferença entre os limites superior e inferior tende a zero mas sua razão tende ao infinito. Contudo, em um importante caso especial o limite superior é exponencialmente melhor, ou seja

\begin{equation*}
	\ln P_e \stackrel{.}{=} -\ln \rho = B, P_e \rightarrow 0
\end{equation*}

%EQUAÇÃO SEM NUMERO

%This happens under the following circumstances (Chernoff ~51) :
Isso ocorre sob as seguintes circunstâncias (Chernoff \cite{r15}):

%1) The components X, of the observation X' = X1,. . . , Xn, are independent and identically distributed

%2).f [pl(x)]l-t [pz(z) ]%x is symmetric in t

%3) N is large.
\begin{enumerate}

	\item Os componentes $X_i$ da observação $ X' = X_1,...,X_N $ são independentes e identicamente distribuídos.
	\item $\int [p_1(x)]^{1-t}[p_2(x)]^tdx $ é simétrico  em $t$.
	\item $N$ é grande.

\end{enumerate}

%We shall not prove this result here. We should note that similar results are encountered in the study of error probability bounds for discrete-memoryless-channels (Gallager [25]). The upper bound in (49) appears to be good even in some cases where the conditions just given are not met (see e.g., Example B in the last section), but no general statement seems to be possible.
Nós não iremos provar este resultado aqui. Devemos notar que resultados similares são encontrados no estudo de limites de probabilidade de erros para canais discretos e sem memória (Gallager \cite{r25}). O limite superior em (\ref{eq49}) parece ser bom mesmo em algumas casos em que as condições que acabamos de dar não são encontradas (veja, por exemplo, o Exemplo B na última Seção), mas não parece ser possível fazer nenhuma afirmação geral.

\subsection{A Divergência e a Probabilidade de Erro}

%The upper bound on PC in terms of p is quite useful. No similar bound in terms of the divergence J appears to be generally true. However by using an inequality between p and J we can obtain a crude lower bound on P, in terms of J. The basic inequalities (apparently first derived by Hoeffding and Wolfowitz [as]) are
O limite superior $P_e$ em termos de $\rho$ é bem útil. Nenhum limite similar em razão da divergência $J$ parece ser geralmente verdadeiro. Contudo, usando uma desigualdade entre $\rho$ e $J$ podemos obter um limite inferior bruto de $P_e$ em termos de $J$. As desigualdades básicas (que foram aparentemente derivadas primeiro por Hoeffding e Wolfowitz \cite{r26}) são

%EQUAÇÃO 50
\begin{equation}
	2B = - \ln \rho \leq I(1,2) \text{ e } 2B \leq I(2,1)
	\label{eq50}
\end{equation}

%where I (1, 2) and I (2, 1) are the Kullback-Leibler(K-L) numbers defined earlier [cf. (2)-(3)) From (50), we get
onde $I(1,2)$ e $I(2,1)$ são os números de Kullback-Leibler (K-L) definidos anteriormente [veja (\ref{eq2})-(\ref{eq3})]. A partir de (\ref{eq50}), temos

%EQUAÇÃO 51
\begin{equation}
	4B \leq J \text{ ou } \rho \geq \exp({J/4})
	\label{eq51}
\end{equation}

%The proofs of (50) follow via Jensen's equality
As provas de (\ref{eq50}) seguem pela igualdade de Jensen

%EQUAÇÃO SEM NUMERO
\begin{equation*}
	\begin{split}
		- \frac{1}{2}I(1,2) = \int p_1(x) \ln & \sqrt{\frac{p_2(x)}{p_1(x)}}dx = E_1 [ \ln\ sqrt{\frac{p_2(x)}{p_1(x)}} ] \\
		& \ln [E_1(\sqrt{p_2(x)/p_1(x)})] = \ln \rho
	\end{split}
\end{equation*}

%A crude lower bound on PC now follows from (49) and (51)
Um limite inferior bruto de $P_e$ agora parte de (\ref{eq49}) e (\ref{eq51})

%EQUAÇÃO 52
\begin{equation}
	P_e \geq \rho^2/8 \geq \frac{1}{8} e^{J/2}
	\label{eq52}
\end{equation}

%Of course, in particular cases, e.g., when pl( .) and pz( .: are Gaussian, we can obtain tighter relations between I: and J than are provided by (51). We shall note some oi these special relations when we give explicit formulas for 6 and J below. To close the present discussion of err01 bounds, we note that the divergence and the I<-L number! are quite useful in providing lower bounds on the conditional error probabilities. These follow from two formula: given by Kullback [SI.
Claro que, em casos particulares, por exemplo quando $p_1(.)$ e $p_2(.)$ são Gaussianas, podemos obter relações mais estreitas entre $B$ e $J$ do que as providas por (\ref{eq51}). Poderemos notar algumas destas relações especiais quando dermos fórmulas explícitas para $B$ e $J$ abaixo. Para fechar a presente discussão sobre limites de erro, notamos que a divergência e os números de K-L são úteis em prover limites inferiores nas probabilidades de erro condicionais. Estas seguem as duas fórmulas dadas por Kullback \cite{r8}.

%EQ 53
\begin{equation}\label{eq53}
	\begin{split}
		I(1,2) \geq P_e^1 \ln (\frac{P_e^1}{1 - P_e^2}) & + \\
		& (1 - P_e^1) \ln (\frac{1 - P_e^1}{P_e^2})
	\end{split}
\end{equation}

%and
e

%EQ 54
\begin{equation}\label{eq54}
	\begin{split}
		I(2,1) \geq P_e^2 \ln (\frac{P_e^2}{1 - P_e^1}) & + \\
		& (1 - P_e^2) \ln (\frac{1 - P_e^2}{P_e^1})
	\end{split}
\end{equation}

%Note that for a fixed value of Pel( P,z}, (53), ( (54)) provides a lower bound on Pez{ Pel ). The RHS in (53) and (54) have interesting interpretations-t,hey are the values of I(1,2) andI(2,l) for binomial distributions with paranleters Peland (1 - Pc2). Tables for the calculations in (53) and (54)are given by Kullback [SI, (see pp. 74-75 and pp. 375-379).
Note que para um valor fixo de $P_{e1}\{P_{e2}\}$, (\ref{eq53}), {(\ref{eq54})} porvê um limite inferior em $P_{e1}\{P_{e2}\}$. O RHS em (\ref{eq53}) e (\ref{eq54}) têm interpretações interessantes - eles são os valores de $I(1,2)$ e $I(2,1)$ para distribuições binomiais com parâmetros $P_{e1}$ e $1-P_{e2}$. Tabelas para cálculos em (\ref{eq53}) e (\ref{eq54}) são dadaw por Kullback \cite{r8}, (veja pag. $74-75$ e $378-379$).

%As with the Bhattacharyya di:stance, the bounds provided by (53)-(54) are exponentially optimum under certain conditions. Thus if x’ xl,. . . ,xN where the xi, i = 1,. . . N are independent and identically ‘distributed, it has been shown that
Ao passo que com a distância de Bhattacharyya, os limites providos por (\ref{eq53}) e (\ref{eq54}) são exponencialmente ótimos sob certas condições. Portanto se $x' = x_1, ...,x_N$ onde os $x_i, i=1,...,N$ são independentes e identicamente distribuídos, foi mostrado que

%EQ SEM NUMERO
\begin{equation*}
	\text{para qualquer valor de }P_e^{(1)}, \lim_{N \rightarrow \infty} - \frac{\ln P_e^{(2)} }{N} = I(1,2) 
\end{equation*}

%where
onde

%EQ SEM NUMERO
\begin{equation*}
	I(1,2) = \int p_1(x) \ln [\frac{p_1(x)}{p_2(x)}]dx
\end{equation*}

%A similar statement holds with 1 and 2 interchanged. These results mere first given by Stein (unpublished); a proof appears in Kullback [SI, pp. 74-77. (See also Chernoff [27]). Daly [as] has applied these results to some problems of radar signal design.
Uma afirmação similar se mantém mesmo com $1$ e $2$ trocados. Estes resultados foram dados primeiramente por Stein (não publicado); uma prova aparece em Kullback \cite{r8}. (Veja também Chernoff \cite{r27}). Daly \cite{r28} aplicou estes resultados de \textit{design} de sinal de radar.

\subsection{Relação de $\rho$ com Informação de Fisher}

%In problems of parameter estimation, we usually have not just two but a continuum c4 hypotheses. For such problems, a special information .measure, introduced by Fisher in 1925, has been used in 1;he statistical literature, see, e.g., Kullback [SI, p. 26. It turns out that the Bhattacharyya distance and the divergence reduce to the Fisher information in estimation problems. To show this, we now let p(+) = a probability density function depending upon a real-valued parameter e, 0 5 e :; 1, say. Then returning to our geometric picture, (41), wme can calculate
Em problemas de estimação de parâmetros, nós normalmente temos não apenas duas, mas um contínuo de hipóteses. Para tais problemas, uma medida de informação especial, introduzida por Fisher em 1925 tem sido usada na literatura estatística, veja por exemplo Kullback \cite{r8}, p. $26$. Acontece que a distância de Bhattacharyya e a divergência são reduzidas a problemas de estimação de informação de Fisher. Para mostrar isso, tomemos $p(x|\theta)=$ uma função densidade de probabilidade dependendo de um parâmetro nos números reais $\theta, 0 \leq \theta \leq 1$, digamos. Voltando à nossa figura geométrica (\ref{eq41}), nós podemos calcular

%EQ 55
\begin{equation}\label{eq55}
	\begin{split}
	\cos \delta s & = \int \sqrt{p(x|\theta)p(x|\theta+ \delta\theta)}dx \\
	& \stackrel{.}{=} 1 - \frac{(\delta \theta)^2}{8} \int_{-\infty}^{\infty} \frac{[\dot{p}(x|\theta)]^2}{p(x|\theta)} dx
	\end{split}
\end{equation}

%where &/e) = (d/dx)p(xle) and 6s = actual length between the points with direction cosiness
em que $\dot{p}(x|\theta) = (d/dx)\dot{p}(x|\theta)$ e $\delta s = $ é a distância real entre pontos com cossenos de direção

%EQ 56
\begin{equation}
	\sqrt{p(x|\theta)} \text{ e } \sqrt{p(x|\theta+ \delta\theta)}
	\label{eq56}
\end{equation}

%In the limit as 6s and 68 tend to zero we get
No limite enquanto $\delta s$ e $\delta \theta$ tendem a zero nós obtemos

%EQ 57
\begin{equation}\label{eq57}
	\begin{split}
		(\frac{ds}{d\theta})^2 & = \frac{1}{4} \int_{-\infty}^{\infty} \frac{[\dot{p}(x|\theta)]^2}{p(x|\theta)} dx \\
		& = - \frac{1}{4} \int_{-\infty}^{\infty} p(x|\theta) [\frac{\partial^2}{\partial \theta^2} \ln p(x|\theta)] d\theta
	\end{split}
\end{equation}

%This derivation is due to Bhattacharyya [12]. A similar calculation for the divergence12 can be found in Kullback [SI, p. 55. The somewhat unexpected appearance of the Fisher information in certain singular detection problems has recently been noted by Shepp [as].
Esta derivação é devida a Bhattacharyya \cite{r12}. Um cálculo similar para a divergência\footnote[12]{A variedade da medida de distância parece ter uma propriedade similar para pequenas variações} pode ser encontrado em Kullback \cite{r8}, pag. $55$. A aparição de certo modo inesperada da informação de Fisher em certos problemas de detecção singular foi notada recentemente por Shepp \cite{r29}.

\subsection{Fórmulas Explícitas para B e J}

%We can calculate B explicitly for a large class of distribuTions-the exponential family. Before giving the general formula we list a few important special cases.
Nós podemos calcular $B$ explicitamente para uma grande classe de distribuição - a família exponencial. Antes de dar a fórmula geral nós listaremos alguns casos especiais importante.

\subsubsection{Distribuições Multinomiais}

%eq 58
\begin{equation}\label{eq58}
	\begin{split}
		\text{Se } p_1(x) = \sum_{j=1}^{N}p_j^{(i)}\delta(x-j), \\
		\text{então } B = -\ln [\sum_{j=1}^{N} \sqrt{p_j^{(1)}p_j^{(2)}}]
	\end{split}
\end{equation}

\subsubsection{Distribuições de Poisson}

%EQ 59
\begin{equation}\label{eq59}
	\begin{split}
		\text{Se } p_1(x) = e^{m_i . \frac{(m_i)^n}{n!}\delta(x-n)}, \\
		\text{então } B = - \frac{1}{2} (\sqrt{m_1}-\sqrt{m_2})^2
	\end{split}
\end{equation}

\subsubsection{Distribuições Gaussianas Univariadas}

%If pi(.) = N(ml, ul), then

%EQ 60
\begin{equation}\label{eq60}
	\begin{split}
		\text{Se } p_1(x) = N(\mu_1,\sigma_1), \\
		\text{então } B = \frac{1}{4} \frac{(m_1-m_2)^2}{\sigma_1^2-\sigma_2^2} + \frac{1}{2} \ln \{\frac{\sigma_1^2+\sigma_2^2}{2\sigma_1\sigma_2}\}
	\end{split}
\end{equation}

\subsubsection{Distribuições Gaussianas Multivariadas}

%If pi@) = N(m,, R,), then
%$Se $p_i(x) = N(\mu_1,R_1)$, então

%EQ 61
\begin{equation}\label{eq61}
	\begin{split}
		\text{Se } p_1(x) = N(\mu_1,R_1), \\
		\text{então } B = \frac{1}{8} (m_1-m_2)'R^{1}(m_1-m_2) \\
		 + \frac{1}{2} \ln \{\frac{det R}{\sqrt{det R_1. det R_2}}\}
	\end{split}
\end{equation}

%where
em que

%EQ 62
\begin{equation}
	2R = R_1 + R_2
	\label{eq62}
\end{equation}

%Huzurbazar [30] has ca.lculated B for exponential densities.
Huzurbazar \cite{r30} calculou $B$ para densidades exponenciais.

\subsubsection{Densidades Exponenciais}

%Let pi(.>
Digamos que $p_i(x)$

%EQ 63
\begin{equation}
	 = d(x)b(h_i) \exp{[\sum_{j-1}^{r} V_j(h_i)U_j(x)]}, i = 1,2.
	\label{eq63}
\end{equation}

%The V,(h,) are independent functions of hi, or more precisely, of the V parameters of the density pL(.). Since the b(h,) also depend upon hi, me can express t.he b(hi) in terms of the V,(h,) as
As $V_i(h_i)$ são funções independentes de $h_i$, ou mais precisamente, de $V$ parâmetros de densidade $p_i(.)$. Como $b(h_i)$ também depende de $h_i$, nós podemos expressar $b(h_i)$ em termos de $V_i(h_i)$, como

%EQ 64
\begin{equation}
	b(h_i) = g\{ [V_j(h_i)] \}
	\label{eq64}
\end{equation}

%[Note: g depends on the hi only through the Vj(hi), j = 1,. . .TI. Then the Bhattacharyya distance is
[Nota: $g$ depende de $h_i$ apenas através de $V_j(h_i), j = 1...,r$]. Então a distância de Bhattacharyya é

%EQ 65
\begin{equation}
	B = - \ln \rho, \rho = \frac{\sqrt{b(h_j)b(h_i)}}{g (\{\frac{1}{2}V_j(h_1) +\frac{1}{2}V_j(h_2) \})}
	\label{eq65}
\end{equation}

\subsubsection{A Divergência}

%Huzurbazar [30] also calculates the divergence J for the exponential clensities
Huzurbazar \cite{r30} também calcula a divergência $J$ para as densidades exponenciais

%eq 66
\begin{equation}
	J = \sum_{j = 1}^{r} [V_j(h_2)-V_j(h_1)][E_2V_j(x)-E_1V_j(x)]
	\label{eq66}
\end{equation}

%where
onde

%eq 67
\begin{equation}\label{eq67}
	\begin{split}
		J = &- \sum_{j = 1}^{r} \sum_{k = 1}^{r} \frac{\partial^2}{\partial V_j \partial V_k} \times \\
		& \{ V_j(h_2) - V_j(h_1)\}\{ V_k(h_2) - V_k(h_1)\}
	\end{split}
\end{equation}

%We give one special case because of its importance.
Nós damos um caso especial por causa de sua importânca.

%If pi(x) = N(nzi, Ri), then
Se $p_i(x) = N(\mu_1,R_1)$, então

%EQ 68
\begin{equation}\label{eq68}
	\begin{split}
		I(1,2) = & \frac{1}{2} \ln \frac{det R_2}{det R_1} + \frac{1}{2} tr R_1[R_2^{-1} - R_1^{-1}] + \\
		& \frac{1}{2} tr R_2^{-1}[m_1 - m_2][m_1 - m_2]'
	\end{split}	
\end{equation}


%and
e

%eq 69
\begin{equation}\label{eq69}
	\begin{split}
		J(1,2) = & \frac{1}{2} tr [R_1-R_2][R_2^{-1} - R_1^{-1}] + \\
		& \frac{1}{2} tr [R_1^{-1} + R_2^{-1}][m_1 - m_2][m_1 - m_2]'
	\end{split}	
\end{equation}

%If the covariances are equal, say, R, = Rz = R, we have
Se as covariâncias são iguais a, digamos, $R_1=R_2=R$, temos

%eq 70
\begin{equation}
	2I(1,2) = J = tr R^{-1}[m_1 - m_2][m_1 - m_2]' \footnote[13]{Esta é a famosa Estatística $D^2$ de Mahalanobis.}
	\label{eq70}
\end{equation}

%We note that the Bhattacharyya distance in this case is
Notamos que a distância de Bhattacharyya neste caso é

%eq 71
\begin{equation}
	B = \frac{J}{8}
	\label{eq71}
\end{equation}

%Whether or not R1 = R2, we can establish the inequality [which is better by a factor of two than the general inequality (Sl)]
Quer ou não que $R_1 = R_2$, podemos estabelecer a desigualdade [que é melhor por um fator de dois que a desigualdade geral (\ref{eq51})]

%eq 72
\begin{equation}
	J \geq 8B
	\label{eq72}
\end{equation}

%The simplest proof of (72) is obtained by considering the case R1 = -1: a diagonal matrix with entries Xi, i = 1,. . .N and R, = I, the identity matrix. (The general case can be obtained from this by using a nonsingular matrix to simultaneously diagonalize R, and Rz.) Then from (67) and (75) we will have, after some algebra,
A prova mais simples de (\ref{eq72}) é obtida considerando o caso de $R_1=\Lambda$, uma matriz diagonal com as entradas $X_i, i=1,...,N$ e $R_2=I$, a matriz identidade. (O caso geral pode ser obtido a partir daí utilizando uma matriz não singular para diagonalizar simultaneamente $R_1$ e $R_2$.) Daí, a partir de (\ref{eq67}) e (\ref{eq75}) nós obtemos, após alguma álgebra,

%eq sem numero
\begin{equation*}
	8B - J = 2\sum \ln(1 + \lambda)^2/4\lambda_i - \sum (\lambda_i - 1)^2/2\lambda_i
\end{equation*}

%But from the inequality In (1 + X) < X, we have
Mas da desigualdade $ln (1+\lambda) < \lambda$ nós temos

%eq 73
\begin{equation}
	\ln(1 + \lambda)^2/4\lambda = \ln(1 + [(\lambda-1)^2/4\lambda]) \leq (\lambda-1)^2/4\lambda
	\label{eq73}
\end{equation}

%so that, using this for each Xi, we get
tal que, utilizando isto para cada $X_i$ nós obtemos

%eq sem numero
\begin{equation*}
	8B - J \leq 0
\end{equation*}

\subsubsection{Equações Diferenciais para B e J}

%Recently there' has been interest (Schweppe [31], Schweppe and Athans [32]), in applying the techniques of control theory to signal design. To do this, it is convenient to obtain differential equations (in continuous-time) for the performance criterion. Schweppe has used B and J for these criteria and obtained differential equations for them when the signal and noise process are projections of Gaussian-Markov processes. The detailed formulas are too complicated to reproduce here, but they can be found in Schweppe [31]. However, we may note that for Gaussian processes with unequal covariances the differential equations for J are so much more complicated than the equations for B that Schweppe does not put down the equations for J. To quote him, "readers who subscribe to the theory that 'the best answer is the simplest answer' may decide that the Bhattacharyya distance is superior to the divergence."
Recentemente tem aparecido interesse (Schweppe [31], Schweppe and Athans [32]), em aplicar técnicas de teoria de controle a design de sinais. Para fazer isto, é conveniente obter equações diferenciais (no tempo-contínuo) como critério de desempenho. Scweppe usou $B$ e $J$ para estes critérios e obteve equações diferenciais para eles quando o sinal e o ruído são projeções de processos de Gauss-Markov. As fórmulas detalhadas são muito complicadas para reproduzir aqui, mas podem ser encontradas em Schweppe [31]. Contudo, nós podemos notar que para processos Gaussianos com covariâncias desiguais as equações diferenciais para $J$ são tão mais complicadas que as equações para $B$ que Schweppe nem coloca equações para $J$. Para citá-lo, "leitores que compartilham da teoria que 'a melhor resposta é a mais simples' irão decidir que a distância de Bhattacharyya é superior à divergência."

\section{Outras Aplicações e Extensões para Tempo-contínuo} \label{sec5}

%The Bhattacharyya distance has found several applica-tions in classical statistics (see, e.g., the papers by Matusita [33], Hannan [34], Rao [35]. A recent application is by Stein [36], p. 17. Stein's result is that the increase in the Bayes risk in a decision problem when an incorrect prior distribution, say a', is used on the hypotheses is bounded (under sufficient regularity conditions) by a constant times the Bhattacharyya distance between d and the true prior distribution a. (This result also extends to prior distributions on unknown parameters.)
A distância de Bhattacharyya encontrou várias aplicações em estatística clássica (veja, por exemplo, os artigos de Matusita \cite{r33}, Hannan \cite{r34}, Rao \cite{r35}. Uma aplicação recente é por Stein \cite{r36}, pag. 17. O resultado de Stein é que o aumento no risco de Bayes em uma problema de decisão quando uma distribuição prior incorreta, digamos $\pi'$, é usada em uma hipótese é limitado (sob condições de regularidade suficientes) por uma constante vezes a distância de Bhattacharyya entre $\pi'$ e a distribuição correta $\pi$. Este resultado também se estende a distribuições priores com parâmetros desconhecidos.)

%We have treated only the two-hypothesis case. In the M-hypothesis case, it appears that the average Bhattacharyya distance
Nós tratamos apenas do caso de duas hipóteses aqui. No caso de M hipóteses, parece que a distância média de Bhattacharyya

%eq sem numero
\begin{equation*}
	\overline{B} = \sum_{i,j}^{M} \pi^{(i)}\pi^{(j)} \int_{-\infty}^{\infty} \sqrt{p_1(x)p_2(x)} dx
\end{equation*}

%would be a suitable measure for signal selection. This average measure will have the same minimax property discussed by Grettenberg [lo] for the average divergence. Furthermore it appears (cf. Gallager [25]), (87)) that, for low P,, B essentially determines the error probability, P,,in the ill-hypothesis case.
seria uma medida aceitável para seleção de sinal. Esta medida média terá a mesma propriedade minimax  discutida por Grettenberg \cite{r10} para a divergência média. Além disso parece (como visto em Gallager \cite{r25}) que , para baixos $P_e$, $B$ essencialmente determina a probabilidade de erro, $P_e$, no caso de M hipóteses.
 
%We close with some comments on the continuous-time case. The usual method of solving continuous-time problems is to start with an approximate discrete-time problem and then obtain the final answer as a limit of the solution to the discrete-time problem. The same procedure works here with one modification. If we set X(ti) = xi, i = 1,. . . N, then the Bhattacharyya distance when observations are restricted to { ti, i = 1, . . . N } is
Nós fechamos com alguns comentários sobre o caso de tempo contínuo. O método usual de resolver problemas no tempo contínuo é começar com um problema aproximado no tempo discreto e obter a resposta final como o limite da solução do problema no tempo discreto. O mesmo procedimento funciona aqui com uma modificação. Se nós temos $X_{(t_i)} = x_i, i = 1,...,N $ , então a distância de Bhattacharyya quando as observações são restritas a $ \{ t_i, i = 1, ..., N\}$ é

%eq 74
\begin{equation}
	B_N = -\ln \int \sqrt{p_{1N}(x)p_{2N}(x)}dx
	\label{eq74}
\end{equation}

%where
onde

%eq 75
\begin{equation} \label{eq75}
	\begin{split}
		p_{1N}(x) - &\text{função densidade de probabilidade} \\
		& \text{de } N \text{variáveis aleatórias} x(t_i), i = 1,...,N
	\end{split}
\end{equation}

%As N+ a, lim piN(x) is generally not defined.
Quando $N \rightarrow \infty, \lim_{N \rightarrow \infty} p_{iN}(x) $ normalmente não é definido.

%However, the ratios
Contudo, as razões

% eq 76
\begin{equation}
	\lim_{N \rightarrow \infty} q_{iN}(x) = \lim_{N \rightarrow \infty} \frac{p_{iN}(x)}{p_{iN}(x) + p_{2N}(x)}
	\label{eq76}
\end{equation}

%will be well behaved (if the sets { ti 1 are monotone increasing) as N --f a. Therefore, in the continuous-time case we should define B as
serão bem comportadas (se os conjuntos $\{t_i\}$ são monotonicamente crescentes), ao passo que $N \rightarrow \infty$. Contudo, no tempo contínuo devemos definir $B$ como

% eq 77
\begin{equation}
		B = - \ln \rho, \text{onde } \rho = \lim_{N \rightarrow \infty} \int \sqrt{q_{1N}(x)q_{2N}(x)}dx
	\label{eq77}
\end{equation}

%Similar arguments can be used to define J in the continuous-time case. With this modification, all the relations and properties that we have obtained in Sections I1 and 111 will hold also for the case of continuous-time observations. The relations
Argumentos similares podem ser usados para definir $J$ no caso de tempo contínuo. Com esta modificação, todas as relações e propriedades que obtivemos nas Seções \ref{sec3} e \ref{sec4} permanecem também para o caso de observações no tempo contínuo. As relações

%eq 78
\begin{equation}
	\frac{1}{8} \rho^2 \leq P_e \leq \frac{1}{2} \rho
	\label{eq78}
\end{equation}

%provide a simple criterion for singular detection, i.e., being able to discriminate between two p~rocesses with arbitrarily low error probability. We see thak a sufficient condition, first noted by Kraft [24], for (extreme) singular detection is that p = 0. The Bhattacharyya distance can also be used to provide a simple proof of a result of Hajek [37] that the detection problem for two Gaussian processes for which the divergence J = co is singular. The proof follows essentially from the fact that in the Gaussian case we can supplement the lower bound (51) on J(J 2 4B) by an upper bound on J of the form J 5 (constant) .B. This and some other aspects of the interplay between B and J in questions of singular detection will be discussed in a separate note.
provêm um critério simples para detecção singular, ou seja, permitem discriminar entre dois processos com uma probabilidade de erro arbitrariamente baixa. Nós vemos que uma condição suficiente, primeiramente notada por Kraft \cite{r24}, para detecção singular (extrema) é a de $\rho = 0$. A distância de Bhattacharyya também pode ser usada para prover uma prova simples de um resultado de Hajek [37] que o problema de detecção para dois processos Gaussianos para os quais a divergência $J = \infty$ é singular. A prova parte essencialmente do fato que no caso Gaussiano nós podemos suplementar o limite inferior (\ref{eq51}) em $J(J \geq 4B)$ por um limite superior em $J$ na forma de $J \leq (constante)\times B$. Este e outros aspectos da interação entre $B$ e $J$ em questões de detecção singular serão discutidas em uma nota separada.

\begin{thebibliography}{99}

\bibitem {r1} Tildesley, "A first study of the Burmese skull", \textit{Biometrika}, vol 13, pp. 176-262, 1921.
\bibitem {r2} P. C. Mahalanobis, "Analysis of race mixture in Bengal", \textit{J. Asiat. Soc. (India)}, vol. 23, pp. 301-310, 1925.
\bibitem {r3} P. C. Mahalanobis, "On the generalized distance in statistics", \textit{Proc. Natl. Inst. Sci. (India)}, vol. 12, pp. 49-55, 1936.
\bibitem {r4} R. A. Fisher, "The use of multiple measurements in taxonomic problems", \textit{Ann. Eugmcis}, vol. 7, pp. 179-188, 1936; \textit{Contributions to Mathematical Statistics}. New York: Wiley, 1950.
\bibitem {r5} T. W. Anderson, \textit{An Introduction to Multiuariate Statistical Analysis}. New York: Wiley, 1955.
\bibitem {r6} C. R. Rao, \textit{Advanced Statisticul Methods in the Estimation of Statistical Parameters}. New York: Wiley, 1952.
\bibitem {r7} H. Jeffreys, "An invariant form for the prior probability in estimation problems", \textit{Proc. Roy. Soc. A.}, vol. 186, pp. 453-461, 1946.
\bibitem {r8} S. Kullback, \textit{Information Theory and Statistics}. New York: Wiley, 1959.
\bibitem {r9} T. Marill and D. M. Green, "On the effectiveness of receptors In recognition systems", \textit{IEEE Trans. on Information Theory}, vol. IT-9, pp. 11-17, January 1963.
\bibitem {r10} T. L. Grettenberg, "Signal selection in communication and radar systems", \textit{IEEE Trans. on Information Theory}, vol. IT-9, pp. 265-275, October 1963.
\bibitem {r11} G. P. Hingorani and J. C. Hancock, "A transmitted reference system for communication in random or unknown channels", \textit{IEEE Trans. on Communication Technology}, vol. COM-13, pp. 293-301, September 1965.
\bibitem {r12} A. Bhattacharyya, "On a measure of divergence between two statistical populations defined by thew probability distributions", \textit{Bull. Calcutta Math. Soc.}, vol, 35, pp. 99-109, 1943.
\bibitem {r13} S. Kakutani, "On equivalence of rnfinlte product measures", \textit{Ann. Math. Stat.}, vol. 49, pp. 214-2?24, 1948.
\bibitem {r14} E. Hellinger, "Neue begrundung der theorie quadrátischer formen von uendlichvielen veránderlichen", \textit{J. fur die Reine und angew. Math.}, vol. 36. pp. 210-271, 1909.
\bibitem {r15} H. Chernoff, "A measure of asymptotic efficiency for tests of hypothesis based on a sum of observations", \textit{Ann. Math. Stat.}, vol. 23, pp. 493-507, 1952.
\bibitem {r16} H. Jeffreys, \textit{Theory of Probability}. Oxford University Press, 1948.
\bibitem {r17} S. Kullback and 12. A. Leibler, "Ch inform:ition and sufficiency," \textit{Ann. Math. Stat.}, vol. 22, pp. 79-86, 1951.
\bibitem {r18} S. Karlin and R. N. Bradt, "On the design and comparison of dichotomous experlments," \textit{Ann. Math. Stat.}, Val. 27, PP.
\bibitem {r19} D. Blackwell, "Comparison of experiments", \textit{Proc. Second Berkeley Symp. on Probability and Statistics}. Berkeley, Calif.:University of California Press, vol. 1, pp. 93-102, 1951.
\bibitem {r20} B. Reiffen and H. Sherman, "An optimum demodulator for Poisson process: photon source detectors" \textit{Proc. IEEE}, vol. 51, pp. 1316-1320, October 1963.
\bibitem {r21} J. N. Pierce, "Theoretical limitations of frequency and time diversity for fading binary transmissions", \textit{IRE Trans. on Communications Systems (Corres.)}, vol. CS-9, pp. 186-157, June 1961.
June 1961.
\bibitem {r22} K. Abend, "Optimum photon detection", \textit{IEEE Trans. on Information Theory}, vol. IT-12, pp. 64-65, January 1966.
\bibitem {r23} [23] B. P. Adhikari and D. D. Joshi, "Distance discrimation et resume exhaustif", \textit{Publs. Inst. Statist.}, vol. 5, pp. 57-74, 1956.
\bibitem {r24} C. H. Kraft, "Some conditions for consistency and uniform consistency of statistical procedures", \textit{University of California Publications in Statistics}, 1955.
\bibitem {r25} R. G. Gallager, "A simple derivation of the coding theorem and some applications", \textit{IEEE Trans. on Information Theory}, vol. IT-11, pp. 3-18, January 1965.
\bibitem {r26} W. Hoeffding and J. Wolfowitz, "Distinguishability of sets of distributions", \textit{Ann. Math. Stat.}, vol. 29, pp. 700-718, 1958.
\bibitem {r27} H. Chernoff, "Large-sample theory: parametric case", \textit{Ann. Math. Stat.}, vol. 27, pp. 1-22, 1956.
\bibitem {r28} R. F. Daly, "Signal design for efficient detection in randomly dispersive media", SRI Tech. Rept. on Project 186531-144, 1965.
\bibitem {r29} L. A. Shepp, "Distinguishing a sequence of random variables from a translate of itself", \textit{Ann. Math. Stat.}, vol. 36, pp. 1107-1112, 1965.
\bibitem {r30} V. S. Huzurbazar, "Exact forms of some invariants for distributions admitting sufficient statistics", \textit{Biometrika}, vol. 42, pp. 533-537, 1955.
\bibitem {r31} F. Schweppe, "On the distance between Gaussian processes: the state space approach", \textit{Information and Control}, 1967.
\bibitem {r32} F. Schweppe and M. Athans, "On the design of optimal modulation schemes via control-theoretic concepts I: formulation", to be published.
\bibitem {r33} K. Matusita, "Decision rules, based on the distance for problems of fit, two samples and estimation", \textit{Ann. Math. Stat.},vol. 26, pp. 631-640, 1955.
\bibitem {r34} J . Hannan, "Consistency of maximum likelihood estimation of discrete distributions", in \textit{Contributions to Probabiltty and Statistics}, I. Olkin, Ed. Stanford, Calif.: Stanford University Press, 1960.
\bibitem {r35} C. R. Rao, "Asymptotic efficiency and limiting information", \textit{Proc. Fourth Berkeley Symp. on Probability and Statistics}. Berkeley, Calif.: University of California Press, vol. 1, pp. 531-545, 1961.
\bibitem {r36} C. Stein, "Approximation of improper prior measures bg Prior probability measures", Department of Statistics, Stanford University, Stanford, Calif., Tech. Rept. 12, 1964.
\bibitem {r37} J. Hajek, "On a property of normal distribution of any stochastic process" (in Russian), \textit{Czech. Math. J.}, vel. 83, pp. 610-618, 1958; a translation appears in “Selected Translations” in \textit{Math. Statistics und Probability}, vol. 1, pp. 245-252.





\end{thebibliography}

%\bibliography{bibliografia}
%\bibliographystyle{ieeetr}

\end{document}

